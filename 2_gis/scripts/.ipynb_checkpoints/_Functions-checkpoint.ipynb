{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions for ArcGIS workflows\n",
    "\n",
    "Author: Adrian Wiegman, adrian.wiegman@usda.gov\n",
    "\n",
    "Created: 10:05 AM Thursday, March 30, 2023\n",
    "\n",
    "Updated: April 18, 2023\n",
    "\n",
    "Notes and Instructions:\n",
    "\n",
    "This script contains helper functions for arcgis workflow to analyze land use and other datasets within watersheds. \n",
    "\n",
    "- prefix all user defined functions with `fn_`\n",
    "- place working functions in the main program\n",
    "- place broken functions in the appendix and comment out or convert cell to Raw NBConvert\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Program\n",
    "\n",
    "functions in this section have been checked and debugged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type `fn_`+TAB to for autocomplete suggestions\n"
     ]
    }
   ],
   "source": [
    "print('type `fn_`+TAB to for autocomplete suggestions')\n",
    "\n",
    "def fn_get_info(name='fn_get_info'):\n",
    "    '''\n",
    "    returns the source information about a given function name\n",
    "    '''\n",
    "    ??$name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this codeblock appends a path to source codes to the environment variable paths\n",
    "# then runs a script containing other source codes\n",
    "import sys\n",
    "# Insert the path of modules folder \n",
    "sys.path.append(r\"C:\\Users\\Adrian.Wiegman\\Documents\\GitHub\\Wiegman_USDA_ARS\\MEP\\scripts\")\n",
    "# Import the module0 directly since \n",
    "# the current path is of modules.\n",
    "%run -m _FeatureTableToDataFrame \n",
    "# the line above runs the script named _FeaturedTableToDataFrame.py from within the path .../scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_run_script_w_propy_bat(\n",
    "    py_script_path=None, # full path to python script includeing the name e.g. \"C:\\hello.py\"\n",
    "    propy_bat_path=\"C:\\Progra~1\\ArcGIS\\Pro\\\\bin\\Python\\Scripts\"\n",
    "    ):\n",
    "    '''\n",
    "    this function can be used to execute standalone python scripts in the ArcGIS python environment using the command line\n",
    "    the benefit of this is that arcpy can be used without opening arcgis\n",
    "    read more: https://pro.arcgis.com/en/pro-app/latest/arcpy/get-started/using-conda-with-arcgis-pro.htm\n",
    "    '''\n",
    "    import os\n",
    "    # create temporary file for testing the function\n",
    "    if py_script_path is None:\n",
    "        import tempfile\n",
    "        tmpdir = tempfile.TemporaryDirectory()\n",
    "        py_script_path = os.path.join(tmpdir.name,\"hello.py\")\n",
    "            # Open the file for writing.\n",
    "        with open(py_script_path, 'w') as f:\n",
    "            f.write(\"print('Hello World!')\")\n",
    "\n",
    "    # get current working directory\n",
    "    wdr = os.getcwd()\n",
    "    \n",
    "    # change directory to folder containing propy.bat\n",
    "    os.chdir(propy_bat_path) \n",
    "    \n",
    "    # construct cmd\n",
    "    cmd = \"propy.bat {}\".format(py_script_path)\n",
    "    \n",
    "    print(\"running command:\\n\")\n",
    "    print(\"{}\\{}\".format(propy_bat_path,cmd))\n",
    "    # execute cmd\n",
    "    os.system(cmd)\n",
    "    \n",
    "    # change directory back \n",
    "    os.chdir(wdr)\n",
    "#fn_run_script_w_propy_bat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_try_mkdir(dirname):\n",
    "    '''\n",
    "    tries to make a new directory\n",
    "    uses proper error handling \n",
    "    '''\n",
    "    import os, errno\n",
    "    try:\n",
    "        os.mkdir(dirname)\n",
    "    except OSError as exc:\n",
    "        if exc.errno != errno.EEXIST:\n",
    "            raise\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_hello(x=\"world\"):\n",
    "    '''\n",
    "    prints hello x\n",
    "    '''\n",
    "    print(\"hello %s\" %x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_recursive_glob_search (startDir=None,\n",
    "                             fileExt=\"csv\"):\n",
    "    '''returns:\n",
    "           file paths matching extension \n",
    "           within all subdirectories starting directory\n",
    "       inputs:\n",
    "           startDir = root or parent directory to start search\n",
    "           fileExt = file extension, e.g. \".csv\" \".xlsx\" \".shp\"\n",
    "    '''\n",
    "    import glob, os\n",
    "    if startDir is None:\n",
    "        startDir = os.getcwd\n",
    "    fileList = []\n",
    "    glbsearch = os.path.join(startDir,'**/*'+fileExt)\n",
    "    for f in glob.glob(glbsearch, recursive=True):\n",
    "        #print(f)\n",
    "        fileList.append(f)\n",
    "    return(fileList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_regex_search_replace(string,pattern,replacement=None):\n",
    "    '''\n",
    "    returns the a string with a pattern substituted by a replacement\n",
    "    '''\n",
    "    if replacement is None: replacement = \"\"\n",
    "    import re\n",
    "    x = re.sub(pattern,replacement,string)\n",
    "    return(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_regex_search_0 (string,pattern,noneVal=\"NA\"):\n",
    "    '''\n",
    "    returns the first match of a regular expression pattern search on a string\n",
    "    '''\n",
    "    import re\n",
    "    x = re.search(pattern,string)\n",
    "    if x is None: \n",
    "        x= [noneVal]    \n",
    "    return(x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_arcpy_table_to_excel(inFeaturePath,outTablePath,outTableName):\n",
    "    import os\n",
    "    arcpy.conversion.TableToExcel(inFeaturePath, os.path.join(outTablePath,outTableName), \"ALIAS\", \"CODE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_agg_sum_df_on_group(group_cols,df,func=sum):\n",
    "    '''\n",
    "    returns data frame aggregated on set of group_cols for given a func\n",
    "    '''\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    return df.groupby(group_cols).aggregate(func).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_add_prefix_suffix_to_selected_cols(df,col_names,prefix=None,suffix=None,sep='_'):\n",
    "    # use list comprehension to add prefix and/or suffix to old names\n",
    "    _ = [i if prefix is None else prefix+sep+i for i in col_names]\n",
    "    new_names = [i if suffix is None else i+sep+suffix for i in _]\n",
    "    df.rename(columns=dict(zip(col_names, new_names)), inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_calc_pct_cover_within_groups(group_cols,x,area_col='Shape_Area'):\n",
    "    '''\n",
    "    calculates percent cover normalize metrics by polygon area\n",
    "    inputs:\n",
    "        x = pandas dataframe containing the following columns\n",
    "        group_cols = a list of strings containing group column names\n",
    "        area_col = string containing the name of the shape area column\n",
    "    all other columns must be numeric columns containing areas of various attribute types \n",
    "    all other columns must have the same units as area_col\n",
    "    '''\n",
    "    x\n",
    "    # copy the numeric columns that are not groups or the selected area column \n",
    "    _ = x.loc[:, ~x.columns.isin(group_cols+[area_col])]._get_numeric_data()\n",
    "    \n",
    "    # divide the \n",
    "    y = _.div(x[area_col],axis=0).mul(100)\n",
    "    if 'Shape_Length' in y.columns:\n",
    "        y.rename(columns={'Shape_Length':'Shape_Perim_to_Area'},inplace=True)\n",
    "    # merge the data back together\n",
    "    z = pd.merge(x[group_cols],x[area_col],left_index=True,right_index=True).merge(y,left_index=True, right_index=True)\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_buildWhereClauseFromList(table, # name of table\n",
    "                                field, # name of field to search\n",
    "                                valueList # list of values for the SQL query\n",
    "                               ):\n",
    "    \n",
    "    \"\"\"Takes a list of values and constructs a SQL WHERE\n",
    "    clause to select those values within a given field and table.\"\"\"\n",
    "\n",
    "    # Add DBMS-specific field delimiters\n",
    "    fieldDelimited = arcpy.AddFieldDelimiters(arcpy.Describe(table).path, field)\n",
    "\n",
    "    # Determine field type\n",
    "    fieldType = arcpy.ListFields(table, field)[0].type\n",
    "\n",
    "    # Add single-quotes for string field values\n",
    "    if str(fieldType) == 'String':\n",
    "        valueList = [\"'%s'\" % value for value in valueList]\n",
    "\n",
    "    # Format WHERE clause in the form of an IN statement\n",
    "    whereClause = \"%s IN(%s)\" % (fieldDelimited, ', '.join(map(str, valueList)))\n",
    "    return whereClause"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get extract the cell size of raster\n",
    "def fn_FA_to_Q (rasterpath=None,recharge_rate_in_yr = 27.25):\n",
    "    _ = arcpy.GetRasterProperties_management(rasterpath, \"CELLSIZEX\")\n",
    "    #Get the elevation standard deviation value from geoprocessing result object\n",
    "    cellsize_x = _.getOutput(0)\n",
    "    _ = arcpy.GetRasterProperties_management(rasterpath, \"CELLSIZEY\")\n",
    "    cellsize_y = _.getOutput(0)\n",
    "    # calculate cell area in meters\n",
    "    cell_area_meters = float(cellsize_x) * float(cellsize_y)\n",
    "    print(\"cell area {} square meters\".format(cell_area_meters))\n",
    "    FA_to_Q = cell_area_meters * recharge_rate_in_yr * 2.54 * (1/100) * (1/365.25)\n",
    "    print(\"FA_to_Q = {} m3/d per cell\".format(FA_to_Q))\n",
    "    return(FA_to_Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_alter_field_double(intablepath,oldname,newname):\n",
    "    '''\n",
    "    renames a field in a table if the field is a double\n",
    "    '''\n",
    "    arcpy.management.AlterField(\n",
    "        in_table=intablepath,\n",
    "        field=oldname,\n",
    "        new_field_name=newname,\n",
    "        new_field_alias=\"\",\n",
    "        field_type=\"DOUBLE\",\n",
    "        field_length=16,\n",
    "        field_is_nullable=\"NULLABLE\",\n",
    "        clear_field_alias=\"CLEAR_ALIAS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_return_float(x):\n",
    "    \"\"\"\n",
    "    returns an object of type float\n",
    "    \"\"\"\n",
    "    try: \n",
    "        return(float(x))\n",
    "    except:\n",
    "        return(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_classify_wetlands(x):\n",
    "    \"\"\"\n",
    "    Consolidates DEP wetlands into fewer categories\n",
    "    \"\"\"\n",
    "    if \"MARSH\" in x:\n",
    "        return(\"MARSH\")\n",
    "    elif \"WOOD\" in x:\n",
    "        return(\"FORESTED SWAMP\")\n",
    "    elif \"SHRUB\" in x:\n",
    "        return(\"SHRUB SWAMP\")\n",
    "    elif len(x) == 0:\n",
    "        return(\"NON WETLAND\")\n",
    "    else:\n",
    "        return(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Broken Functions\n",
    "\n",
    "place broken functions in this section and set as `Raw NBConvert`"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# this cell works, but not in the arcpy python environment. \n",
    "import pandas as pd\n",
    "import statsmodels.formula.api as smf \n",
    "\n",
    "def fn_get_vif(exogs, data):\n",
    "    '''Return VIF (variance inflation factor) DataFrame\n",
    "\n",
    "    Args:\n",
    "    exogs (list): list of exogenous/independent variables\n",
    "    data (DataFrame): the df storing all variables\n",
    "\n",
    "    Returns:\n",
    "    VIF and Tolerance DataFrame for each exogenous variable\n",
    "\n",
    "    Notes:\n",
    "    Assume we have a list of exogenous variable [X1, X2, X3, X4].\n",
    "    To calculate the VIF and Tolerance for each variable, we regress\n",
    "    each of them against other exogenous variables. For instance, the\n",
    "    regression model for X3 is defined as:\n",
    "                        X3 ~ X1 + X2 + X4\n",
    "    And then we extract the R-squared from the model to calculate:\n",
    "                    VIF = 1 / (1 - R-squared)\n",
    "                    Tolerance = 1 - R-squared\n",
    "    The cutoff to detect multicollinearity:\n",
    "                    VIF > 10 or Tolerance < 0.1\n",
    "    '''\n",
    "\n",
    "    # initialize dictionaries\n",
    "    vif_dict, tolerance_dict = {}, {}\n",
    "\n",
    "    # create formula for each exogenous variable\n",
    "    for exog in exogs:\n",
    "        not_exog = [i for i in exogs if i != exog]\n",
    "        formula = f\"{exog} ~ {' + '.join(not_exog)}\"\n",
    "\n",
    "        # extract r-squared from the fit\n",
    "        r_squared = smf.ols(formula, data=data).fit().rsquared\n",
    "\n",
    "        # calculate VIF\n",
    "        vif = 1/(1 - r_squared)\n",
    "        vif_dict[exog] = vif\n",
    "\n",
    "        # calculate tolerance\n",
    "        tolerance = 1 - r_squared\n",
    "        tolerance_dict[exog] = tolerance\n",
    "\n",
    "    # return VIF DataFrame\n",
    "    df_vif = pd.DataFrame({'VIF': vif_dict, 'Tolerance': tolerance_dict})\n",
    "\n",
    "    return df_vif\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def fn_load_TaxParAssess_data():\n",
    "    df = pd.read_pickle(\"outputs/MEP_TaxParAssess.pkl\")\n",
    "    return(df)\n",
    "def fn_join_df_TaxParAssess_on_LOC_ID(df,df_TaxParAssess=None):\n",
    "    if DF_Assess is None:\n",
    "        df = fn_load_TaxParAssess_data()\n",
    "    df.merge(left_df,right_df,on)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ArcGISPro",
   "language": "Python",
   "name": "python3"
  },
  "language_info": {
   "file_extension": ".py",
   "name": "python",
   "version": "3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
